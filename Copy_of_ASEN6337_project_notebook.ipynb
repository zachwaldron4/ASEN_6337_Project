{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C5VY5Bz0HQV0",
    "outputId": "510876b0-1540-43aa-fee5-3bdb71aa80c8"
   },
   "outputs": [],
   "source": [
    "# # Google collab-specific import version protection\n",
    "# try:\n",
    "#   # %tensorflow_version only exists in Colab.\n",
    "#   %tensorflow_version 2.x\n",
    "# except Exception:\n",
    "#     pass\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "bJK9U6uzGum6",
    "outputId": "6eb1a1a0-be1a-453e-e557-ad20f32f0d52"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-aef10f5b388a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#this is a comment\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import albumentations as albu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate\n",
    "\n",
    "!pip install segmentation-models\n",
    "import segmentation_models as sm\n",
    "\n",
    "import os, glob\n",
    "import cv2\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from albumentations import Compose, VerticalFlip, HorizontalFlip, Rotate, GridDistortion,CenterCrop\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from numpy.random import seed\n",
    "seed(10)\n",
    "#from tensorflow import set_random_seed\n",
    "#set_random_seed(10)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "PF2r9O9xjRwz",
    "outputId": "8b56e189-cdba-4b11-c188-7d716a420eab"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q kaggle\n",
    "!mkdir -p ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQE6nenyjZQj"
   },
   "outputs": [],
   "source": [
    "!touch kaggle.json\n",
    "!echo '{\"username\":\"grantberland\",\"key\":\"1e151670e705336403f0919b24130ded\"}' > kaggle.json\n",
    "!cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "N2v02oSxkTNU",
    "outputId": "c2d40afa-ef85-46ee-95c1-e7137b60d8a4"
   },
   "outputs": [],
   "source": [
    "!kaggle competitions download -c understanding_cloud_organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cbxNRf8nktGu",
    "outputId": "666f5913-2508-4790-d1e9-a4b6cd146286"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "!mkdir test_images\n",
    "!mkdir train_images\n",
    "\n",
    "!ls\n",
    "\n",
    "import zipfile\n",
    "\n",
    "if os.listdir('test_images') and os.listdir('train_images') is None:\n",
    "    with zipfile.ZipFile('test_images.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('test_images')\n",
    "\n",
    "    with zipfile.ZipFile('train_images.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('train_images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "j5oXn3zUt8q0",
    "outputId": "4f6c42b7-1f5d-416c-d457-666598edbf88"
   },
   "outputs": [],
   "source": [
    "# LETS GET TO WORK BOYS\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "test_imgs_folder = '/test_images/'\n",
    "train_imgs_folder = '/train_images/'\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "train_df = pd.read_csv('train.csv.zip', compression='zip')\n",
    "train_df.head()\n",
    "\n",
    "train_df = train_df[~train_df['EncodedPixels'].isnull()]\n",
    "train_df['Image'] = train_df['Image_Label'].map(lambda x: x.split('_')[0])\n",
    "train_df['Class'] = train_df['Image_Label'].map(lambda x: x.split('_')[1])\n",
    "classes = train_df['Class'].unique()\n",
    "train_df = train_df.groupby('Image')['Class'].agg(set).reset_index()\n",
    "# for class_name in classes:\n",
    "#     train_df[class_name] = train_df['Class'].map(lambda x: 1 if class_name in x else 0)\n",
    "\n",
    "\n",
    "# img_2_ohe_vector = {img:vec for img, vec in zip(train_df['Image'], train_df.iloc[:, 2:].values)}\n",
    "\n",
    "# train_imgs, val_imgs = train_test_split(train_df['Image'].values, \n",
    "#                                         test_size=0.1, \n",
    "#                                         stratify=train_df['Class'].map(lambda x: str(sorted(list(x)))), # sorting present classes in lexicographical order, just to be sure\n",
    "#                                         random_state=43)\n",
    "# train_df.head()\n",
    "for class_name in classes:\n",
    "    train_df[class_name] = train_df['Class'].map(lambda x: 1 if class_name in x else 0)\n",
    "\n",
    "\n",
    "####\n",
    "train_df2 = train_df.sample(1000)\n",
    "\n",
    "img_2_ohe_vector = {img:vec for img, vec in zip(train_df2['Image'], train_df2.iloc[:, 2:].values)}\n",
    "\n",
    "train_imgs, val_imgs = train_test_split(train_df2['Image'].values, \n",
    "                                        test_size=0.1, \n",
    "                                        stratify=train_df2['Class'].map(lambda x: str(sorted(list(x)))), # sorting present classes in lexicographical order, just to be sure\n",
    "                                        random_state=43)\n",
    "train_df2.info()\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dx7RKc5aNlp5"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOKgAgDQuCi_"
   },
   "outputs": [],
   "source": [
    "class DataGenenerator(Sequence):\n",
    "    def __init__(self, images_list=None, folder_imgs=train_imgs_folder, \n",
    "                 batch_size=32, shuffle=True, augmentation=None,\n",
    "                 resized_height=224, resized_width=224, num_channels=3):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentation = augmentation\n",
    "        if images_list is None:\n",
    "            self.images_list = os.listdir(folder_imgs)\n",
    "        else:\n",
    "            self.images_list = deepcopy(images_list)\n",
    "        self.folder_imgs = folder_imgs\n",
    "        self.len = len(self.images_list) // self.batch_size\n",
    "        self.resized_height = resized_height\n",
    "        self.resized_width = resized_width\n",
    "        self.num_channels = num_channels\n",
    "        self.num_classes = 4\n",
    "        self.is_test = not 'train' in folder_imgs\n",
    "        if not shuffle and not self.is_test:\n",
    "            self.labels = [img_2_ohe_vector[img] for img in self.images_list[:self.len*self.batch_size]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def on_epoch_start(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.images_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_batch = self.images_list[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        X = np.empty((self.batch_size, self.resized_height, self.resized_width, self.num_channels))\n",
    "        y = np.empty((self.batch_size, self.num_classes))\n",
    "\n",
    "\n",
    "        for i, image_name in enumerate(current_batch):\n",
    "            path = os.path.join(self.folder_imgs, image_name)\n",
    "            try:\n",
    "                img = cv2.resize(cv2.imread(path), (self.resized_height, self.resized_width)).astype(np.float32)\n",
    "                print(img.shape)\n",
    "            except:\n",
    "              # if it fails, who cares there are more images\n",
    "              img = np.zeros([self.resized_height, self.resized_width, 3], dtype=np.float32)\n",
    "\n",
    "\n",
    "            if not self.augmentation is None:\n",
    "                print(img.shape)\n",
    "                augmented = self.augmentation(image=img)\n",
    "                img = augmented['image']\n",
    "                print(img.shape)\n",
    "            X[i, :, :, :] = img/255.0\n",
    "            if not self.is_test:\n",
    "                y[i, :] = img_2_ohe_vector[image_name]\n",
    "        return X, y\n",
    "\n",
    "    def get_labels(self):\n",
    "        if self.shuffle:\n",
    "            images_current = self.images_list[:self.len*self.batch_size]\n",
    "            labels = [img_2_ohe_vector[img] for img in images_current]\n",
    "        else:\n",
    "            labels = self.labels\n",
    "        return np.array(labels)\n",
    "\n",
    "albumentations_train = Compose([\n",
    "    VerticalFlip(), HorizontalFlip(), Rotate(limit=20), GridDistortion()], p=1)\n",
    "\n",
    "data_generator_train = DataGenenerator(train_imgs, augmentation=None)\n",
    "data_generator_train_eval = DataGenenerator(train_imgs, shuffle=False)\n",
    "data_generator_val = DataGenenerator(val_imgs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lM58tEnLwNw2"
   },
   "outputs": [],
   "source": [
    "# the code below is called at the end of every training epoch to adjust and tweak \n",
    "# hyperparameters in the training process\n",
    "\n",
    "class PrAucCallback(Callback):\n",
    "    def __init__(self, data_generator, num_workers=num_cores, \n",
    "                 early_stopping_patience=3, \n",
    "                 plateau_patience=3, reduction_rate=0.5,\n",
    "                 stage='train', checkpoints_path='checkpoints/'):\n",
    "        super(Callback, self).__init__()\n",
    "        self.data_generator = data_generator\n",
    "        self.num_workers = num_workers\n",
    "        self.class_names = ['Fish', 'Flower', 'Sugar', 'Gravel']\n",
    "        self.history = [[] for _ in range(len(self.class_names) + 1)] # to store per each class and also mean PR AUC\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.plateau_patience = plateau_patience\n",
    "        self.reduction_rate = reduction_rate\n",
    "        self.stage = stage\n",
    "        self.best_pr_auc = -float('inf')\n",
    "        if not os.path.exists(checkpoints_path):\n",
    "            os.makedirs(checkpoints_path)\n",
    "        self.checkpoints_path = checkpoints_path\n",
    "        \n",
    "    def compute_pr_auc(self, y_true, y_pred):\n",
    "        pr_auc_mean = 0\n",
    "        print(f\"\\n{'#'*30}\\n\")\n",
    "        for class_i in range(len(self.class_names)):\n",
    "            precision, recall, _ = precision_recall_curve(y_true[:, class_i], y_pred[:, class_i])\n",
    "            pr_auc = auc(recall, precision)\n",
    "            pr_auc_mean += pr_auc/len(self.class_names)\n",
    "            print(f\"PR AUC {self.class_names[class_i]}, {self.stage}: {pr_auc:.3f}\\n\")\n",
    "            self.history[class_i].append(pr_auc)        \n",
    "        print(f\"\\n{'#'*20}\\n PR AUC mean, {self.stage}: {pr_auc_mean:.3f}\\n{'#'*20}\\n\")\n",
    "        self.history[-1].append(pr_auc_mean)\n",
    "        return pr_auc_mean\n",
    "              \n",
    "    def is_patience_lost(self, patience):\n",
    "        if len(self.history[-1]) > patience:\n",
    "            best_performance = max(self.history[-1][-(patience + 1):-1])\n",
    "            return best_performance == self.history[-1][-(patience + 1)] and best_performance >= self.history[-1][-1]    \n",
    "              \n",
    "    def early_stopping_check(self, pr_auc_mean):\n",
    "        if self.is_patience_lost(self.early_stopping_patience):\n",
    "            self.model.stop_training = True    \n",
    "              \n",
    "    def model_checkpoint(self, pr_auc_mean, epoch):\n",
    "        if pr_auc_mean > self.best_pr_auc:\n",
    "            # remove previous checkpoints to save space\n",
    "            for checkpoint in glob.glob(os.path.join(self.checkpoints_path, 'classifier_epoch_*')):\n",
    "                os.remove(checkpoint)\n",
    "        self.best_pr_auc = pr_auc_mean\n",
    "        self.model.save(os.path.join(self.checkpoints_path, f'classifier_epoch_{epoch}_val_pr_auc_{pr_auc_mean}.h5'))              \n",
    "        print(f\"\\n{'#'*20}\\nSaved new checkpoint\\n{'#'*20}\\n\")\n",
    "              \n",
    "    def reduce_lr_on_plateau(self):\n",
    "        if self.is_patience_lost(self.plateau_patience):\n",
    "            new_lr = float(keras.backend.get_value(self.model.optimizer.lr)) * self.reduction_rate\n",
    "            keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
    "            print(f\"\\n{'#'*20}\\nReduced learning rate to {new_lr}.\\n{'#'*20}\\n\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict_generator(self.data_generator, workers=self.num_workers)\n",
    "        y_true = self.data_generator.get_labels()\n",
    "        # estimate AUC under precision recall curve for each class\n",
    "        pr_auc_mean = self.compute_pr_auc(y_true, y_pred)\n",
    "              \n",
    "        if self.stage == 'val':\n",
    "            # early stop after early_stopping_patience=4 epochs of no improvement in mean PR AUC\n",
    "            self.early_stopping_check(pr_auc_mean)\n",
    "\n",
    "            # save a model with the best PR AUC in validation\n",
    "            self.model_checkpoint(pr_auc_mean, epoch)\n",
    "\n",
    "            # reduce learning rate on PR AUC plateau\n",
    "            self.reduce_lr_on_plateau()            \n",
    "        \n",
    "    def get_pr_auc_history(self):\n",
    "        return self.history\n",
    "\n",
    "train_metric_callback = PrAucCallback(data_generator_train_eval)\n",
    "val_callback = PrAucCallback(data_generator_val, stage='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "rlLHkFcOwckm",
    "outputId": "abac81e3-cfb1-4bfe-82ca-6d5ad66a29dc"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "from keras_applications.resnext import ResNeXt101\n",
    "\n",
    "def get_model1():\n",
    "    base_model = InceptionResNetV2(weights='imagenet', include_top=False, pooling='avg')\n",
    "    x = base_model.output\n",
    "    y_pred = Dense(4, activation='sigmoid')(x)\n",
    "    return Model(inputs=base_model.input, outputs=y_pred)\n",
    "\n",
    "def get_model2():\n",
    "    base_model = DenseNet121(weights='imagenet', include_top=False, pooling='avg')\n",
    "    x = base_model.output\n",
    "    y_pred = Dense(4, activation='sigmoid')(x)\n",
    "    return Model(inputs=base_model.input, outputs=y_pred)\n",
    "\n",
    "def get_model3():\n",
    "    base_model = model = ResNeXt101(..., backend=tf.keras.backend, layers=tf.keras.layers, weights = 'imagenet', models=tf.keras.models, utils=tf.keras.utils)\n",
    "    x = base_model.output\n",
    "    y_pred = Dense(4, activation='sigmoid')(x)\n",
    "    return Model(inputs=base_model.input, outputs=y_pred)\n",
    "\n",
    "# set this variable to train model below\n",
    "model = get_model1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5Belu95CwhlO",
    "outputId": "bd6c6ec1-aeb4-436d-9799-7ed6b0928ee0"
   },
   "outputs": [],
   "source": [
    "for base_layer in model.layers[:-1]:\n",
    "    base_layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy')\n",
    "history_0 = model.fit_generator(generator=data_generator_train,\n",
    "                              validation_data=data_generator_val,\n",
    "                              epochs=15,\n",
    "                              callbacks=[train_metric_callback, val_callback],\n",
    "                              workers=num_cores,\n",
    "                              verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXa8klsYfbyF"
   },
   "outputs": [],
   "source": [
    "# Sequentially built model out of Keras -GB\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "#create model\n",
    "model_test = Sequential()\n",
    "\n",
    "#add model layers\n",
    "model_test.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "model_test.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model_test.add(Flatten())\n",
    "model_test.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#compile model using accuracy to measure model performance\n",
    "model_test.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the simple model\n",
    "model_test.fit_generator(generator=data_generator_train, \n",
    "          validation_data=data_generator_val, \n",
    "          epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KpIl29-vjEuz"
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: need to change variable names\n",
    "score = model_test.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lM_kuDeG0OnH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of ASEN6337_project_notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
